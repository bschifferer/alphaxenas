{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a notebook for a simple benchmark of DenseNet. As this is not the main contribution, this notebook has no comments and/or is well organized. It is a proof-of-concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 1.191127,
     "end_time": "2019-03-09T21:49:05.372464",
     "exception": false,
     "start_time": "2019-03-09T21:49:04.181337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from alphaenas.data_utils import read_data, saveObjWithPickle, createPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.017959,
     "end_time": "2019-03-09T21:49:05.402216",
     "exception": false,
     "start_time": "2019-03-09T21:49:05.384257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = 'data/cifar10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 2.846554,
     "end_time": "2019-03-09T21:49:08.259497",
     "exception": false,
     "start_time": "2019-03-09T21:49:05.412943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Reading data\n",
      "data/cifar10/data_batch_1\n",
      "data/cifar10/data_batch_2\n",
      "data/cifar10/data_batch_3\n",
      "data/cifar10/data_batch_4\n",
      "data/cifar10/data_batch_5\n",
      "data/cifar10/test_batch\n",
      "Prepropcess: [subtract mean], [divide std]\n",
      "mean: [125.34512 122.94169 113.83898]\n",
      "std: [63.02383 62.13708 66.74233]\n"
     ]
    }
   ],
   "source": [
    "images, labels = read_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.02729,
     "end_time": "2019-03-09T21:49:08.299476",
     "exception": false,
     "start_time": "2019-03-09T21:49:08.272186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.023181,
     "end_time": "2019-03-09T21:49:08.336015",
     "exception": false,
     "start_time": "2019-03-09T21:49:08.312834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class ChildModel():\\n    def __init__(self, images, labels, path):\\n        tf.reset_default_graph() \\n        self.width = images['train'].shape[1]\\n        self.height = images['train'].shape[2]\\n        self.channels = images['train'].shape[3]\\n        self.num_classes = len(np.unique(labels['train']))\\n        self.path = path\\n    \\n    def build_model(self):\\n        batch_size = 32\\n        input_data = tf.placeholder(shape=[None, self.width, self.height, self.channels], dtype=tf.float32, name='x_input')\\n        y_output = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='y_output')\\n        bl_training = tf.placeholder(tf.bool, name='training')\\n        \\n        x = tf.layers.conv2d(input_data, filters = 512, kernel_size = (3, 3), padding = 'same')\\n        x = tf.layers.batch_normalization(x, training = bl_training)\\n        x = tf.nn.relu(x)\\n        x = tf.layers.conv2d(input_data, filters = 256, kernel_size = (3, 3), padding = 'same')\\n        x = tf.layers.batch_normalization(x, training = bl_training)\\n        x = tf.nn.relu(x)\\n        x = tf.layers.conv2d(input_data, filters = 128, kernel_size = (3, 3), padding = 'same')\\n        x = tf.layers.batch_normalization(x, training = bl_training)\\n        x = tf.nn.relu(x)\\n        x = tf.layers.conv2d(input_data, filters = 64, kernel_size = (3, 3), padding = 'same')\\n        x = tf.layers.batch_normalization(x, training = bl_training)\\n        x = tf.nn.relu(x)\\n        x = tf.layers.conv2d(input_data, filters = 32, kernel_size = (3, 3), padding = 'same')\\n        x = tf.layers.batch_normalization(x, training = bl_training)\\n        x = tf.nn.relu(x)\\n        \\n        x = tf.layers.flatten(x)\\n        x = tf.layers.dense(x, 1024)\\n        x = tf.layers.batch_normalization(x, training = bl_training)\\n        x = tf.nn.relu(x)\\n        x = tf.layers.dense(x, 1024)\\n        x = tf.layers.batch_normalization(x, training = bl_training)\\n        x = tf.nn.relu(x)\\n        x = tf.layers.dense(x, self.num_classes)\\n        x_softmax = tf.nn.softmax(x)\\n        \\n        y = tf.one_hot(tf.cast(y_output, tf.int32), self.num_classes)\\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y))\\n        \\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\\n        with tf.control_dependencies(update_ops):\\n            optimizer = tf.train.MomentumOptimizer(0.01, 0.9, use_nesterov=True).minimize(loss)\\n        \\n        self.optimizer = optimizer\\n        self.loss = loss\\n        self.input_data = input_data\\n        self.y_output = y_output\\n        self.x = x\\n        self.bl_training = bl_training\\n        self.x_softmax = x_softmax\\n        \\n    def train(self, images, labels):\\n        optimizer = self.optimizer\\n        loss = self.loss\\n        input_data = self.input_data \\n        y_output = self.y_output\\n        x = self.x\\n        bl_training = self.bl_training\\n        x_softmax = self.x_softmax\\n        batch_size = 32\\n        path = self.path\\n        \\n        createPath(path)\\n        x_train = images['train']\\n        y_train = labels['train']\\n        x_test = images['test']\\n        y_test = labels['test']\\n        y_train = y_train.reshape((-1, 1))\\n        y_test = y_test.reshape((-1, 1))\\n        \\n        with tf.Session() as sess:\\n            sess.run(tf.global_variables_initializer())\\n            idx = np.asarray(range(x_train.shape[0]))\\n            for e in range(8):\\n                no_batches = idx.shape[0]//batch_size + 1\\n                np.random.shuffle(idx)\\n                for batch in range(no_batches-1):\\n                    x_train_batch = x_train[idx[(batch*batch_size):(min((1+batch)*batch_size,x_train.shape[0]))]]\\n                    y_train_batch = y_train[idx[(batch*batch_size):(min((1+batch)*batch_size,x_train.shape[0]))]]\\n                    o_loss, o_optimizer, o_x = sess.run([loss, optimizer, x], feed_dict={input_data:x_train_batch, y_output:y_train_batch, bl_training: 1})\\n                \\n                val_o_loss, val_o_x = sess.run([loss, x_softmax], feed_dict={input_data:x_test, y_output: y_test, bl_training: 0})\\n                arg_max = np.argmax(val_o_x, axis=-1)\\n                val_acc = np.mean(arg_max.reshape((-1,1))==y_test)\\n                print('Epoch: ' + str(e) + ' Val Loss: ' + str(val_o_loss) + ' Val Acc: ' + str(val_acc))\\n        \\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class ChildModel():\n",
    "    def __init__(self, images, labels, path):\n",
    "        tf.reset_default_graph() \n",
    "        self.width = images['train'].shape[1]\n",
    "        self.height = images['train'].shape[2]\n",
    "        self.channels = images['train'].shape[3]\n",
    "        self.num_classes = len(np.unique(labels['train']))\n",
    "        self.path = path\n",
    "    \n",
    "    def build_model(self):\n",
    "        batch_size = 32\n",
    "        input_data = tf.placeholder(shape=[None, self.width, self.height, self.channels], dtype=tf.float32, name='x_input')\n",
    "        y_output = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='y_output')\n",
    "        bl_training = tf.placeholder(tf.bool, name='training')\n",
    "        \n",
    "        x = tf.layers.conv2d(input_data, filters = 512, kernel_size = (3, 3), padding = 'same')\n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(input_data, filters = 256, kernel_size = (3, 3), padding = 'same')\n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(input_data, filters = 128, kernel_size = (3, 3), padding = 'same')\n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(input_data, filters = 64, kernel_size = (3, 3), padding = 'same')\n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(input_data, filters = 32, kernel_size = (3, 3), padding = 'same')\n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = tf.layers.flatten(x)\n",
    "        x = tf.layers.dense(x, 1024)\n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.dense(x, 1024)\n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.dense(x, self.num_classes)\n",
    "        x_softmax = tf.nn.softmax(x)\n",
    "        \n",
    "        y = tf.one_hot(tf.cast(y_output, tf.int32), self.num_classes)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y))\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.MomentumOptimizer(0.01, 0.9, use_nesterov=True).minimize(loss)\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.input_data = input_data\n",
    "        self.y_output = y_output\n",
    "        self.x = x\n",
    "        self.bl_training = bl_training\n",
    "        self.x_softmax = x_softmax\n",
    "        \n",
    "    def train(self, images, labels):\n",
    "        optimizer = self.optimizer\n",
    "        loss = self.loss\n",
    "        input_data = self.input_data \n",
    "        y_output = self.y_output\n",
    "        x = self.x\n",
    "        bl_training = self.bl_training\n",
    "        x_softmax = self.x_softmax\n",
    "        batch_size = 32\n",
    "        path = self.path\n",
    "        \n",
    "        createPath(path)\n",
    "        x_train = images['train']\n",
    "        y_train = labels['train']\n",
    "        x_test = images['test']\n",
    "        y_test = labels['test']\n",
    "        y_train = y_train.reshape((-1, 1))\n",
    "        y_test = y_test.reshape((-1, 1))\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            idx = np.asarray(range(x_train.shape[0]))\n",
    "            for e in range(8):\n",
    "                no_batches = idx.shape[0]//batch_size + 1\n",
    "                np.random.shuffle(idx)\n",
    "                for batch in range(no_batches-1):\n",
    "                    x_train_batch = x_train[idx[(batch*batch_size):(min((1+batch)*batch_size,x_train.shape[0]))]]\n",
    "                    y_train_batch = y_train[idx[(batch*batch_size):(min((1+batch)*batch_size,x_train.shape[0]))]]\n",
    "                    o_loss, o_optimizer, o_x = sess.run([loss, optimizer, x], feed_dict={input_data:x_train_batch, y_output:y_train_batch, bl_training: 1})\n",
    "                \n",
    "                val_o_loss, val_o_x = sess.run([loss, x_softmax], feed_dict={input_data:x_test, y_output: y_test, bl_training: 0})\n",
    "                arg_max = np.argmax(val_o_x, axis=-1)\n",
    "                val_acc = np.mean(arg_max.reshape((-1,1))==y_test)\n",
    "                print('Epoch: ' + str(e) + ' Val Loss: ' + str(val_o_loss) + ' Val Acc: ' + str(val_acc))\n",
    "        \n",
    "'''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.01943,
     "end_time": "2019-03-09T21:49:08.368176",
     "exception": false,
     "start_time": "2019-03-09T21:49:08.348746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = ChildModel(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.018877,
     "end_time": "2019-03-09T21:49:08.400218",
     "exception": false,
     "start_time": "2019-03-09T21:49:08.381341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.019532,
     "end_time": "2019-03-09T21:49:08.433043",
     "exception": false,
     "start_time": "2019-03-09T21:49:08.413511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.train(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012452,
     "end_time": "2019-03-09T21:49:08.459125",
     "exception": false,
     "start_time": "2019-03-09T21:49:08.446673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import cv2\n",
    "\n",
    "IMG_WIDTH = 32\n",
    "IMG_HEIGHT = 32\n",
    "IMG_DEPTH = 3\n",
    "NUM_CLASS = 10\n",
    "\n",
    "def horizontal_flip(image, axis):\n",
    "    '''\n",
    "    Flip an image at 50% possibility\n",
    "    :param image: a 3 dimensional numpy array representing an image\n",
    "    :param axis: 0 for vertical flip and 1 for horizontal flip\n",
    "    :return: 3D image after flip\n",
    "    '''\n",
    "    flip_prop = np.random.randint(low=0, high=2)\n",
    "    if flip_prop == 0:\n",
    "        image = cv2.flip(image, axis)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def whitening_image(image_np):\n",
    "    '''\n",
    "    Performs per_image_whitening\n",
    "    :param image_np: a 4D numpy array representing a batch of images\n",
    "    :return: the image numpy array after whitened\n",
    "    '''\n",
    "    for i in range(len(image_np)):\n",
    "        mean = np.mean(image_np[i, ...])\n",
    "        # Use adjusted standard deviation here, in case the std == 0.\n",
    "        std = np.max([np.std(image_np[i, ...]), 1.0/np.sqrt(IMG_HEIGHT * IMG_WIDTH * IMG_DEPTH)])\n",
    "        image_np[i,...] = (image_np[i, ...] - mean) / std\n",
    "    return image_np\n",
    "\n",
    "\n",
    "def random_crop_and_flip(batch_data):\n",
    "    '''\n",
    "    Helper to random crop and random flip a batch of images\n",
    "    :param padding_size: int. how many layers of 0 padding was added to each side\n",
    "    :param batch_data: a 4D batch array\n",
    "    :return: randomly cropped and flipped image\n",
    "    '''\n",
    "    cropped_batch = np.zeros(len(batch_data) * IMG_HEIGHT * IMG_WIDTH * IMG_DEPTH).reshape(\n",
    "        len(batch_data), IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH)\n",
    "\n",
    "    for i in range(len(batch_data)):\n",
    "        x_offset = np.random.randint(low=0, high=2 * 2, size=1)[0]\n",
    "        y_offset = np.random.randint(low=0, high=2 * 2, size=1)[0]\n",
    "        cropped_batch[i, x_offset:x_offset+IMG_HEIGHT,\n",
    "                      y_offset:y_offset+IMG_WIDTH] = batch_data[i, ...][x_offset:x_offset+IMG_HEIGHT,\n",
    "                      y_offset:y_offset+IMG_WIDTH, :]\n",
    "\n",
    "        cropped_batch[i, ...] = horizontal_flip(image=cropped_batch[i, ...], axis=1)\n",
    "\n",
    "    return cropped_batch\n",
    "\n",
    "\n",
    "class ResNetModel():\n",
    "    def __init__(self, images, labels, batch_size, max_iteration, lr, lr_iteration_step = [32000, 42000], path):\n",
    "        tf.reset_default_graph() \n",
    "        self.width = images['train'].shape[1]\n",
    "        self.height = images['train'].shape[2]\n",
    "        self.channels = images['train'].shape[3]\n",
    "        self.num_classes = len(np.unique(labels['train']))\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iteration = max_iteration\n",
    "        self.lr_iteration_step = lr_iteration_step\n",
    "        self.lr = lr\n",
    "        self.regularizer = tf.contrib.layers.l2_regularizer(scale=0.0002)\n",
    "        self.path = path\n",
    "    \n",
    "    def bn_relu_conv_layer(self,\n",
    "                           x,\n",
    "                           bl_training,\n",
    "                           filter_num, \n",
    "                           filter_size, \n",
    "                           stride = (1, 1),\n",
    "                           padding = 'same'\n",
    "                          ):\n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d(x, filters = filter_num, \n",
    "                             kernel_size = filter_size, \n",
    "                             strides = stride, \n",
    "                             padding = padding,\n",
    "                             kernel_regularizer=self.regularizer)\n",
    "        return(x)\n",
    "    \n",
    "    def residual_block(self,\n",
    "                       i,\n",
    "                       x, \n",
    "                       bl_training,\n",
    "                       filter_num\n",
    "                      ):\n",
    "        \n",
    "        input_channel = x.get_shape().as_list()[-1]\n",
    "        if i == 0:\n",
    "            conv = self.bn_relu_conv_layer(x, bl_training, filter_num, (3,3), stride = (2, 2))\n",
    "        else:\n",
    "            conv = self.bn_relu_conv_layer(x, bl_training, filter_num, (3,3))\n",
    "            \n",
    "        conv = self.bn_relu_conv_layer(conv, bl_training, filter_num, (3,3))\n",
    "        \n",
    "        pooled_input = None\n",
    "        if i == 0:\n",
    "            pooled_input = tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "            padded_input = tf.pad(pooled_input, [[0, 0], [0, 0], [0, 0], [input_channel // 2, input_channel // 2]])\n",
    "        else:\n",
    "            padded_input = x\n",
    "\n",
    "        output = conv + padded_input\n",
    "        return(output)     \n",
    "    \n",
    "    def build_model(self):\n",
    "        batch_size = self.batch_size\n",
    "        n = 5\n",
    "        input_data = tf.placeholder(shape=[None, self.width, self.height, self.channels], dtype=tf.float32, name='x_input')\n",
    "        y_output = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='y_output')\n",
    "        bl_training = tf.placeholder(tf.bool, name='training')\n",
    "        learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "        \n",
    "        x = tf.layers.conv2d(input_data, filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_regularizer=self.regularizer)\n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters = 16, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_regularizer=self.regularizer)\n",
    "        conv = self.bn_relu_conv_layer(x, bl_training, 16, (3,3))\n",
    "        x = conv + x\n",
    "        \n",
    "        print('First block')\n",
    "        for i in range(1, n):\n",
    "            print(i)\n",
    "            x = self.residual_block(i, x, bl_training, 16)\n",
    "        \n",
    "        print('Second block')\n",
    "        for i in range(n):\n",
    "            print(i)\n",
    "            x = self.residual_block(i, x, bl_training, 32)\n",
    "        \n",
    "        print('Third block')\n",
    "        for i in range(n):\n",
    "            print(i)\n",
    "            x = self.residual_block(i, x, bl_training, 64)\n",
    "        \n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = tf.reduce_mean(x, axis=[1,2])\n",
    "        x = tf.layers.dense(x, 10)\n",
    "        x_softmax = tf.nn.softmax(x)\n",
    "        \n",
    "        y = tf.one_hot(tf.cast(y_output, tf.int32), self.num_classes)\n",
    "        l2_loss = tf.losses.get_regularization_loss()\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y))\n",
    "        loss += l2_loss\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True).minimize(loss)\n",
    "            #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "            #optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.5).minimize(loss)\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.input_data = input_data\n",
    "        self.y_output = y_output\n",
    "        self.x = x\n",
    "        self.bl_training = bl_training\n",
    "        self.x_softmax = x_softmax\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def train(self, images, labels):\n",
    "        optimizer = self.optimizer\n",
    "        loss = self.loss\n",
    "        input_data = self.input_data \n",
    "        y_output = self.y_output\n",
    "        x = self.x\n",
    "        bl_training = self.bl_training\n",
    "        x_softmax = self.x_softmax\n",
    "        batch_size = self.batch_size\n",
    "        max_iteration = self.max_iteration\n",
    "        lr_iteration_step = self.lr_iteration_step\n",
    "        lr = self.lr\n",
    "        learning_rate = self.learning_rate\n",
    "        path = self.path\n",
    "        \n",
    "        createPath(path)\n",
    "        x_train = images['train']\n",
    "        y_train = labels['train']\n",
    "        x_test = images['test']\n",
    "        y_test = labels['test']\n",
    "        y_train = y_train.reshape((-1, 1))\n",
    "        y_test = y_test.reshape((-1, 1))\n",
    "        \n",
    "        iteration = 0\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            idx = np.asarray(range(x_train.shape[0]))\n",
    "            for e in range(100000):\n",
    "                no_batches = idx.shape[0]//batch_size + 1\n",
    "                np.random.shuffle(idx)\n",
    "                for batch in range(no_batches-1):\n",
    "                    x_train_batch = x_train[idx[(batch*batch_size):(min((1+batch)*batch_size,x_train.shape[0]))]]\n",
    "                    x_train_batch = random_crop_and_flip(x_train_batch)\n",
    "                    y_train_batch = y_train[idx[(batch*batch_size):(min((1+batch)*batch_size,x_train.shape[0]))]]\n",
    "                    o_loss, o_optimizer, o_x = sess.run([loss, optimizer, x], feed_dict={input_data:x_train_batch, \n",
    "                                                                                         y_output:y_train_batch, \n",
    "                                                                                         bl_training: 1,\n",
    "                                                                                         learning_rate: lr})\n",
    "                    iteration += 1\n",
    "                    if iteration == max_iteration:\n",
    "                        break\n",
    "                    if iteration in lr_iteration_step:\n",
    "                        lr = 0.1 * lr\n",
    "                        print('New learning rate: ' + str(lr))\n",
    "                        \n",
    "                val_o_loss, val_o_x = sess.run([loss, x_softmax], feed_dict={input_data:x_test, y_output: y_test, bl_training: 0})\n",
    "                arg_max = np.argmax(val_o_x, axis=-1)\n",
    "                val_acc = np.mean(arg_max.reshape((-1,1))==y_test)\n",
    "                print('Epoch: ' + str(e) + ' Iteration: ' + str(iteration) + ' Val Loss: ' + str(val_o_loss) + ' Val Acc: ' + str(val_acc))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.019443,
     "end_time": "2019-03-09T21:49:08.491813",
     "exception": false,
     "start_time": "2019-03-09T21:49:08.472370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = ResNetModel(images, \n",
    "#                    labels,\n",
    "#                    batch_size = 128, \n",
    "#                    max_iteration = 64000, \n",
    "#                    lr = 0.1\n",
    "#                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.019564,
     "end_time": "2019-03-09T21:49:08.524729",
     "exception": false,
     "start_time": "2019-03-09T21:49:08.505165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.018813,
     "end_time": "2019-03-09T21:49:08.556805",
     "exception": false,
     "start_time": "2019-03-09T21:49:08.537992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.train(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 0.116478,
     "end_time": "2019-03-09T21:49:08.699286",
     "exception": false,
     "start_time": "2019-03-09T21:49:08.582808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "IMG_WIDTH = 32\n",
    "IMG_HEIGHT = 32\n",
    "IMG_DEPTH = 3\n",
    "NUM_CLASS = 10\n",
    "\n",
    "def horizontal_flip(image, axis):\n",
    "    '''\n",
    "    Flip an image at 50% possibility\n",
    "    :param image: a 3 dimensional numpy array representing an image\n",
    "    :param axis: 0 for vertical flip and 1 for horizontal flip\n",
    "    :return: 3D image after flip\n",
    "    '''\n",
    "    flip_prop = np.random.randint(low=0, high=2)\n",
    "    if flip_prop == 0:\n",
    "        image = cv2.flip(image, axis)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def whitening_image(image_np):\n",
    "    '''\n",
    "    Performs per_image_whitening\n",
    "    :param image_np: a 4D numpy array representing a batch of images\n",
    "    :return: the image numpy array after whitened\n",
    "    '''\n",
    "    for i in range(len(image_np)):\n",
    "        mean = np.mean(image_np[i, ...])\n",
    "        # Use adjusted standard deviation here, in case the std == 0.\n",
    "        std = np.max([np.std(image_np[i, ...]), 1.0/np.sqrt(IMG_HEIGHT * IMG_WIDTH * IMG_DEPTH)])\n",
    "        image_np[i,...] = (image_np[i, ...] - mean) / std\n",
    "    return image_np\n",
    "\n",
    "\n",
    "def random_crop_and_flip(batch_data):\n",
    "    '''\n",
    "    Helper to random crop and random flip a batch of images\n",
    "    :param padding_size: int. how many layers of 0 padding was added to each side\n",
    "    :param batch_data: a 4D batch array\n",
    "    :return: randomly cropped and flipped image\n",
    "    '''\n",
    "    cropped_batch = np.zeros(len(batch_data) * IMG_HEIGHT * IMG_WIDTH * IMG_DEPTH).reshape(\n",
    "        len(batch_data), IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH)\n",
    "\n",
    "    for i in range(len(batch_data)):\n",
    "        x_offset = np.random.randint(low=0, high=2 * 2, size=1)[0]\n",
    "        y_offset = np.random.randint(low=0, high=2 * 2, size=1)[0]\n",
    "        cropped_batch[i, x_offset:x_offset+IMG_HEIGHT,\n",
    "                      y_offset:y_offset+IMG_WIDTH] = batch_data[i, ...][x_offset:x_offset+IMG_HEIGHT,\n",
    "                      y_offset:y_offset+IMG_WIDTH, :]\n",
    "\n",
    "        cropped_batch[i, ...] = horizontal_flip(image=cropped_batch[i, ...], axis=1)\n",
    "\n",
    "    return cropped_batch\n",
    "\n",
    "\n",
    "class DenseNetModel():\n",
    "    def __init__(self, images, labels, batch_size, max_iteration, lr, path, lr_iteration_step = [15000, 30000]):\n",
    "        tf.reset_default_graph() \n",
    "        self.width = images['train'].shape[1]\n",
    "        self.height = images['train'].shape[2]\n",
    "        self.channels = images['train'].shape[3]\n",
    "        self.num_classes = len(np.unique(labels['train']))\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iteration = max_iteration\n",
    "        self.lr_iteration_step = lr_iteration_step\n",
    "        self.lr = lr\n",
    "        self.regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001)   \n",
    "        self.path = path\n",
    "    \n",
    "    def dense_block(self, x, nb_layers, growth_rate, bottleneck, wd, p, bl_training=None):\n",
    "        if bottleneck: \n",
    "            nb_layers //= 2\n",
    "        for i in range(nb_layers):\n",
    "            b = self.conv_block(x, growth_rate, bottleneck=bottleneck, p=p, wd=wd, bl_training=bl_training)\n",
    "            x = tf.concat([x,b], axis=-1)\n",
    "        return x\n",
    "    \n",
    "    def conv_block(self, x, nf, bottleneck=False, p=None, wd=0, bl_training=None):\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        if bottleneck:\n",
    "            x = tf.layers.conv2d(x, filters = nf*4, kernel_size = (1, 1), strides = (1, 1), padding = 'same', kernel_regularizer=self.regularizer)\n",
    "            x = tf.layers.dropout(x, rate=p, training=bl_training)\n",
    "            x = tf.nn.relu(x)\n",
    "            x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        x = tf.layers.conv2d(x, filters = nf, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_regularizer=self.regularizer)\n",
    "        x = tf.layers.dropout(x, rate=p, training=bl_training)\n",
    "        return(x)\n",
    "    \n",
    "    def transition_block(self, x, compression=1.0, p=None, wd=0, bl_training=None):\n",
    "        nf = int(x.get_shape().as_list()[-1] * compression)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "        x = tf.layers.conv2d(x, filters = nf, kernel_size = (1, 1), strides = (1, 1), padding = 'same', kernel_regularizer=self.regularizer)\n",
    "        x = tf.layers.dropout(x, rate=p, training=bl_training)\n",
    "        x = tf.layers.average_pooling2d(x, (2, 2), (2, 2))\n",
    "        return(x)\n",
    "    \n",
    "    def build_model(self):\n",
    "        batch_size = self.batch_size\n",
    "        n = 5\n",
    "        depth = 100\n",
    "        nb_filter = 16\n",
    "        compression = 0.5\n",
    "        bottleneck = True\n",
    "        p = 0.2\n",
    "        wd = 1e-4\n",
    "        nb_block=3\n",
    "        growth_rate=12\n",
    "        \n",
    "        input_data = tf.placeholder(shape=[None, self.width, self.height, self.channels], dtype=tf.float32, name='x_input')\n",
    "        y_output = tf.placeholder(shape=[None, 1], dtype=tf.float32, name='y_output')\n",
    "        bl_training = tf.placeholder(tf.bool, name='training')\n",
    "        learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "        \n",
    "        nb_layers_per_block = int((depth - 4) / nb_block)\n",
    "        nb_layers = [nb_layers_per_block] * nb_block\n",
    "        \n",
    "        x = tf.layers.conv2d(input_data, filters = 16, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_regularizer=self.regularizer)\n",
    "        x = tf.layers.dropout(x, rate=p, training=bl_training)\n",
    "        \n",
    "        for i, block in enumerate(nb_layers):\n",
    "            x = self.dense_block(x, block, growth_rate, bottleneck=bottleneck, p=p, wd=wd, bl_training=bl_training)\n",
    "            if i != len(nb_layers)-1:\n",
    "                x = self.transition_block(x, compression=compression, p=p, wd=wd, bl_training=bl_training)\n",
    "        \n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.batch_normalization(x, training = bl_training)\n",
    "                \n",
    "        x = tf.reduce_mean(x, axis=[1,2])\n",
    "        x = tf.layers.dense(x, 10, kernel_regularizer=self.regularizer)\n",
    "        x_softmax = tf.nn.softmax(x)\n",
    "        \n",
    "        y = tf.one_hot(tf.cast(y_output, tf.int32), self.num_classes)\n",
    "        l2_loss = tf.losses.get_regularization_loss()\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y))\n",
    "        loss += l2_loss\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True).minimize(loss)\n",
    "            #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "            #optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.5).minimize(loss)\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.input_data = input_data\n",
    "        self.y_output = y_output\n",
    "        self.x = x\n",
    "        self.bl_training = bl_training\n",
    "        self.x_softmax = x_softmax\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def predict_batch(self, sess, tensors, input_data, y_output, bl_training, tmp_images, tmp_labels, batch_size):\n",
    "        idx_tmp = np.asarray(range(tmp_images.shape[0]))\n",
    "        no_batches_tmp = idx_tmp.shape[0]//batch_size + 1\n",
    "        y_tmp_final = []\n",
    "        x_tmp_pred_final = []\n",
    "        tmp_o_loss_final = []\n",
    "        for batch in range(no_batches_tmp-1):\n",
    "            x_tmp_batch = tmp_images[idx_tmp[(batch*batch_size):(min((1+batch)*batch_size,tmp_images.shape[0]))]]\n",
    "            y_tmp_batch = tmp_labels[idx_tmp[(batch*batch_size):(min((1+batch)*batch_size,tmp_images.shape[0]))]]\n",
    "            tmp_o_loss, tmp_o_x = sess.run(tensors, feed_dict={input_data:x_tmp_batch, y_output: y_tmp_batch, bl_training: 0})\n",
    "            arg_max = np.argmax(tmp_o_x, axis=-1)\n",
    "            x_tmp_pred_final.append(arg_max)\n",
    "            y_tmp_final.append(y_tmp_batch)\n",
    "            tmp_o_loss_final.append(tmp_o_loss)\n",
    "            \n",
    "        return(y_tmp_final, x_tmp_pred_final, tmp_o_loss_final)\n",
    "                \n",
    "        y_test_final = np.asarray(y_test_final)\n",
    "        x_pred_final = np.asarray(x_pred_final)\n",
    "        val_acc = np.mean(x_pred_final.reshape((-1,1))==y_test_final.reshape((-1,1)))\n",
    "        val_o_loss = np.mean(val_o_loss)\n",
    "    \n",
    "    def train(self, images, labels):\n",
    "        optimizer = self.optimizer\n",
    "        loss = self.loss\n",
    "        input_data = self.input_data \n",
    "        y_output = self.y_output\n",
    "        x = self.x\n",
    "        bl_training = self.bl_training\n",
    "        x_softmax = self.x_softmax\n",
    "        batch_size = self.batch_size\n",
    "        max_iteration = self.max_iteration\n",
    "        lr_iteration_step = self.lr_iteration_step\n",
    "        lr = self.lr\n",
    "        learning_rate = self.learning_rate\n",
    "        path = self.path\n",
    "        \n",
    "        createPath(path)\n",
    "        createPath(path + '/model')\n",
    "        x_train = images['train']\n",
    "        y_train = labels['train']\n",
    "        x_val = images['valid']\n",
    "        y_val = labels['valid']\n",
    "        x_test = images['test']\n",
    "        y_test = labels['test']\n",
    "        y_train = y_train.reshape((-1, 1))\n",
    "        y_val = y_val.reshape((-1, 1))\n",
    "        y_test = y_test.reshape((-1, 1))\n",
    "        \n",
    "        iteration = 0\n",
    "        iteration_total = []\n",
    "        val_acc_total = []\n",
    "        test_acc_total = []\n",
    "        total_time_total = []\n",
    "        result = {}\n",
    "        \n",
    "        best_loss = 999\n",
    "        best_epoch = 0\n",
    "        bl_break = False\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            idx = np.asarray(range(x_train.shape[0]))\n",
    "            for e in range(100000):\n",
    "                if bl_break:\n",
    "                    break\n",
    "                start = timer()\n",
    "                no_batches = idx.shape[0]//batch_size + 1\n",
    "                np.random.shuffle(idx)\n",
    "                for batch in range(no_batches-1):\n",
    "                    x_train_batch = x_train[idx[(batch*batch_size):(min((1+batch)*batch_size,x_train.shape[0]))]]\n",
    "                    x_train_batch = random_crop_and_flip(x_train_batch)\n",
    "                    y_train_batch = y_train[idx[(batch*batch_size):(min((1+batch)*batch_size,x_train.shape[0]))]]\n",
    "                    o_loss, o_optimizer, o_x = sess.run([loss, optimizer, x], feed_dict={input_data:x_train_batch, \n",
    "                                                                                         y_output:y_train_batch, \n",
    "                                                                                         bl_training: 1,\n",
    "                                                                                         learning_rate: lr})\n",
    "                    iteration += 1\n",
    "                    if iteration == max_iteration:\n",
    "                        bl_break = True\n",
    "                    if iteration in lr_iteration_step:\n",
    "                        lr = 0.1 * lr\n",
    "                        print('New learning rate: ' + str(lr))\n",
    "                                        \n",
    "                y_val_final, x_val_pred_final, val_o_loss_final = self.predict_batch(sess, \n",
    "                                                                                     [loss, x_softmax],\n",
    "                                                                                     input_data, y_output, bl_training,\n",
    "                                                                                     x_val, \n",
    "                                                                                     y_val, \n",
    "                                                                                     batch_size) \n",
    "                \n",
    "                y_val_final = np.asarray(y_val_final)\n",
    "                x_val_pred_final = np.asarray(x_val_pred_final)\n",
    "                val_acc = np.mean(x_val_pred_final.reshape((-1,1))==y_val_final.reshape((-1,1)))\n",
    "                val_o_loss_mean = np.mean(val_o_loss_final)\n",
    "                \n",
    "                y_test_final, x_test_pred_final, test_o_loss_final = self.predict_batch(sess, \n",
    "                                                                                        [loss, x_softmax],\n",
    "                                                                                        input_data, y_output, bl_training,\n",
    "                                                                                        x_test, \n",
    "                                                                                        y_test, \n",
    "                                                                                        batch_size) \n",
    "                \n",
    "                y_test_final = np.asarray(y_test_final)\n",
    "                x_test_pred_final = np.asarray(x_test_pred_final)\n",
    "                test_acc = np.mean(x_test_pred_final.reshape((-1,1))==y_test_final.reshape((-1,1)))\n",
    "                test_o_loss_mean = np.mean(test_o_loss_final)\n",
    "                \n",
    "                if best_loss > val_o_loss_mean:\n",
    "                    print('Safe best model')\n",
    "                    best_loss = val_o_loss_mean\n",
    "                    best_epoch = e\n",
    "                    saver.save(sess, path + '/model/{}'.format('best_model' + str(best_epoch)))\n",
    "                \n",
    "                iteration_total.append(iteration)\n",
    "                val_acc_total.append(val_acc)\n",
    "                test_acc_total.append(test_acc)\n",
    "                \n",
    "                end = timer()\n",
    "                total_time = end-start\n",
    "                total_time_total.append(total_time)\n",
    "                result['iteration_total'] = iteration_total\n",
    "                result['val_acc'] = val_acc_total\n",
    "                result['test_acc'] = test_acc_total\n",
    "                result['time'] = total_time_total\n",
    "                \n",
    "                saveObjWithPickle(result, path + '/result.pickle')\n",
    "                \n",
    "                print('Time: ' + str(total_time) + ' Epoch: ' + str(e) + ' Iteration: ' + str(iteration) + ' Val Loss: ' + str(val_o_loss_mean) + ' Val Acc: ' + str(val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 64719.130619,
     "end_time": "2019-03-10T15:47:47.844330",
     "exception": false,
     "start_time": "2019-03-09T21:49:08.713711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-8c24739b212f>:140: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Safe best model\n",
      "Time: 431.8055562200025 Epoch: 0 Iteration: 351 Val Loss: 1.580794 Val Acc: 0.5667067307692307\n",
      "Safe best model\n",
      "Time: 358.7637650610013 Epoch: 1 Iteration: 702 Val Loss: 1.2038969 Val Acc: 0.6822916666666666\n",
      "Safe best model\n",
      "Time: 358.8891367239994 Epoch: 2 Iteration: 1053 Val Loss: 1.1208892 Val Acc: 0.7069310897435898\n",
      "Time: 355.477563092998 Epoch: 3 Iteration: 1404 Val Loss: 1.1801211 Val Acc: 0.6935096153846154\n",
      "Safe best model\n",
      "Time: 359.3599541420008 Epoch: 4 Iteration: 1755 Val Loss: 0.93183994 Val Acc: 0.7580128205128205\n",
      "Safe best model\n",
      "Time: 358.5475252969991 Epoch: 5 Iteration: 2106 Val Loss: 0.82710904 Val Acc: 0.797676282051282\n",
      "Time: 355.6350153050007 Epoch: 6 Iteration: 2457 Val Loss: 0.84352964 Val Acc: 0.7938701923076923\n",
      "Time: 355.6067929439996 Epoch: 7 Iteration: 2808 Val Loss: 0.82720965 Val Acc: 0.7970753205128205\n",
      "Safe best model\n",
      "Time: 358.62522476100276 Epoch: 8 Iteration: 3159 Val Loss: 0.75135636 Val Acc: 0.8153044871794872\n",
      "Safe best model\n",
      "Time: 358.60965084399504 Epoch: 9 Iteration: 3510 Val Loss: 0.7371153 Val Acc: 0.827323717948718\n",
      "Safe best model\n",
      "Time: 358.7681191329975 Epoch: 10 Iteration: 3861 Val Loss: 0.68808967 Val Acc: 0.8359375\n",
      "Time: 355.6432765810023 Epoch: 11 Iteration: 4212 Val Loss: 0.68878907 Val Acc: 0.8377403846153846\n",
      "Safe best model\n",
      "Time: 358.61622900899965 Epoch: 12 Iteration: 4563 Val Loss: 0.6795844 Val Acc: 0.8473557692307693\n",
      "Safe best model\n",
      "Time: 358.01031288500235 Epoch: 13 Iteration: 4914 Val Loss: 0.6639096 Val Acc: 0.8433493589743589\n",
      "Time: 355.3535444119989 Epoch: 14 Iteration: 5265 Val Loss: 0.697533 Val Acc: 0.8363381410256411\n",
      "Time: 355.139668582 Epoch: 15 Iteration: 5616 Val Loss: 0.67461634 Val Acc: 0.8467548076923077\n",
      "Safe best model\n",
      "Time: 358.3781803059974 Epoch: 16 Iteration: 5967 Val Loss: 0.62315786 Val Acc: 0.8633814102564102\n",
      "Time: 355.32882262399653 Epoch: 17 Iteration: 6318 Val Loss: 0.65532446 Val Acc: 0.8529647435897436\n",
      "Time: 355.1203662070038 Epoch: 18 Iteration: 6669 Val Loss: 0.6682754 Val Acc: 0.8485576923076923\n",
      "Time: 355.44993617300497 Epoch: 19 Iteration: 7020 Val Loss: 0.6335102 Val Acc: 0.8653846153846154\n",
      "Time: 355.2383542769967 Epoch: 20 Iteration: 7371 Val Loss: 0.6432115 Val Acc: 0.8619791666666666\n",
      "New learning rate: 0.010000000000000002\n",
      "Safe best model\n",
      "Time: 358.3143948880024 Epoch: 21 Iteration: 7722 Val Loss: 0.5451223 Val Acc: 0.8872195512820513\n",
      "Safe best model\n",
      "Time: 358.5399329329957 Epoch: 22 Iteration: 8073 Val Loss: 0.5293809 Val Acc: 0.8902243589743589\n",
      "Safe best model\n",
      "Time: 358.391581003998 Epoch: 23 Iteration: 8424 Val Loss: 0.5268064 Val Acc: 0.890625\n",
      "Safe best model\n",
      "Time: 358.4960410410058 Epoch: 24 Iteration: 8775 Val Loss: 0.5231622 Val Acc: 0.8922275641025641\n",
      "Safe best model\n",
      "Time: 358.5008914709979 Epoch: 25 Iteration: 9126 Val Loss: 0.517921 Val Acc: 0.8954326923076923\n",
      "Time: 355.3143711869998 Epoch: 26 Iteration: 9477 Val Loss: 0.52710927 Val Acc: 0.8916266025641025\n",
      "Time: 355.1527228720006 Epoch: 27 Iteration: 9828 Val Loss: 0.51951486 Val Acc: 0.8924278846153846\n",
      "New learning rate: 0.0010000000000000002\n",
      "Safe best model\n",
      "Time: 358.332440872 Epoch: 28 Iteration: 10179 Val Loss: 0.51407593 Val Acc: 0.8978365384615384\n",
      "Safe best model\n",
      "Time: 358.41599130800023 Epoch: 29 Iteration: 10530 Val Loss: 0.51185805 Val Acc: 0.8986378205128205\n",
      "Safe best model\n",
      "Time: 358.55767798000306 Epoch: 30 Iteration: 10881 Val Loss: 0.50955164 Val Acc: 0.8990384615384616\n",
      "Time: 355.2249884099947 Epoch: 31 Iteration: 11232 Val Loss: 0.5096612 Val Acc: 0.9006410256410257\n",
      "Safe best model\n",
      "Time: 358.39745199299796 Epoch: 32 Iteration: 11583 Val Loss: 0.5082976 Val Acc: 0.8994391025641025\n",
      "Safe best model\n",
      "Time: 358.77995011999883 Epoch: 33 Iteration: 11934 Val Loss: 0.50828576 Val Acc: 0.8996394230769231\n",
      "Time: 355.31250028299837 Epoch: 34 Iteration: 12285 Val Loss: 0.50945777 Val Acc: 0.8996394230769231\n",
      "Time: 355.01427569199586 Epoch: 35 Iteration: 12636 Val Loss: 0.5084821 Val Acc: 0.8994391025641025\n",
      "Safe best model\n",
      "Time: 358.7977913880022 Epoch: 36 Iteration: 12987 Val Loss: 0.50665843 Val Acc: 0.8990384615384616\n",
      "Time: 355.2970625730013 Epoch: 37 Iteration: 13338 Val Loss: 0.5067126 Val Acc: 0.8982371794871795\n",
      "Time: 355.1723704719989 Epoch: 38 Iteration: 13689 Val Loss: 0.50680053 Val Acc: 0.8994391025641025\n",
      "Safe best model\n",
      "Time: 358.03664634300367 Epoch: 39 Iteration: 14040 Val Loss: 0.50611675 Val Acc: 0.9000400641025641\n",
      "Time: 355.18334875500295 Epoch: 40 Iteration: 14391 Val Loss: 0.5062448 Val Acc: 0.8994391025641025\n",
      "Time: 354.9320644920008 Epoch: 41 Iteration: 14742 Val Loss: 0.50612736 Val Acc: 0.9002403846153846\n",
      "Safe best model\n",
      "Time: 358.1300772629984 Epoch: 42 Iteration: 15093 Val Loss: 0.5060737 Val Acc: 0.9000400641025641\n",
      "Safe best model\n",
      "Time: 430.0362189939988 Epoch: 0 Iteration: 351 Val Loss: 1.8168049 Val Acc: 0.5356570512820513\n",
      "Safe best model\n",
      "Time: 358.71042152199516 Epoch: 1 Iteration: 702 Val Loss: 1.1965909 Val Acc: 0.6732772435897436\n",
      "Safe best model\n",
      "Time: 358.84927246200095 Epoch: 2 Iteration: 1053 Val Loss: 1.1723616 Val Acc: 0.6794871794871795\n",
      "Safe best model\n",
      "Time: 359.13076049299707 Epoch: 3 Iteration: 1404 Val Loss: 1.0366653 Val Acc: 0.7107371794871795\n",
      "Safe best model\n",
      "Time: 359.08141554800386 Epoch: 4 Iteration: 1755 Val Loss: 0.9922609 Val Acc: 0.7393830128205128\n",
      "Safe best model\n",
      "Time: 358.7828229020015 Epoch: 5 Iteration: 2106 Val Loss: 0.89924973 Val Acc: 0.7662259615384616\n",
      "Safe best model\n",
      "Time: 359.08867059700424 Epoch: 6 Iteration: 2457 Val Loss: 0.82703 Val Acc: 0.7878605769230769\n",
      "Safe best model\n",
      "Time: 358.7808770200063 Epoch: 7 Iteration: 2808 Val Loss: 0.7984544 Val Acc: 0.8048878205128205\n",
      "Safe best model\n",
      "Time: 358.5497188849986 Epoch: 8 Iteration: 3159 Val Loss: 0.73450655 Val Acc: 0.8221153846153846\n",
      "Time: 355.4250124770042 Epoch: 9 Iteration: 3510 Val Loss: 0.8076688 Val Acc: 0.8008814102564102\n",
      "Safe best model\n",
      "Time: 358.3311207730003 Epoch: 10 Iteration: 3861 Val Loss: 0.72315854 Val Acc: 0.8297275641025641\n",
      "Time: 355.3398402460007 Epoch: 11 Iteration: 4212 Val Loss: 0.73272985 Val Acc: 0.8219150641025641\n",
      "Safe best model\n",
      "Time: 358.6741641339977 Epoch: 12 Iteration: 4563 Val Loss: 0.7111704 Val Acc: 0.8369391025641025\n",
      "Safe best model\n",
      "Time: 358.62892137599556 Epoch: 13 Iteration: 4914 Val Loss: 0.68077755 Val Acc: 0.8387419871794872\n",
      "Safe best model\n",
      "Time: 358.54747466600384 Epoch: 14 Iteration: 5265 Val Loss: 0.67343754 Val Acc: 0.8483573717948718\n",
      "Safe best model\n",
      "Time: 357.5292310590012 Epoch: 15 Iteration: 5616 Val Loss: 0.66725606 Val Acc: 0.8447516025641025\n",
      "Safe best model\n",
      "Time: 357.46483365099994 Epoch: 16 Iteration: 5967 Val Loss: 0.62305325 Val Acc: 0.8597756410256411\n",
      "Time: 354.7874427639981 Epoch: 17 Iteration: 6318 Val Loss: 0.70191205 Val Acc: 0.835136217948718\n",
      "Time: 354.5495782910002 Epoch: 18 Iteration: 6669 Val Loss: 0.6398021 Val Acc: 0.8575721153846154\n",
      "Time: 354.5823873680056 Epoch: 19 Iteration: 7020 Val Loss: 0.6374527 Val Acc: 0.8533653846153846\n",
      "Time: 354.7250820670015 Epoch: 20 Iteration: 7371 Val Loss: 0.6486774 Val Acc: 0.8529647435897436\n",
      "New learning rate: 0.010000000000000002\n",
      "Safe best model\n",
      "Time: 358.69789269600005 Epoch: 21 Iteration: 7722 Val Loss: 0.5306071 Val Acc: 0.8896233974358975\n",
      "Safe best model\n",
      "Time: 358.42373961100384 Epoch: 22 Iteration: 8073 Val Loss: 0.5264018 Val Acc: 0.8908253205128205\n",
      "Safe best model\n",
      "Time: 358.2182435709983 Epoch: 23 Iteration: 8424 Val Loss: 0.5178445 Val Acc: 0.8938301282051282\n",
      "Safe best model\n",
      "Time: 358.3325959110007 Epoch: 24 Iteration: 8775 Val Loss: 0.51422876 Val Acc: 0.8956330128205128\n",
      "Safe best model\n",
      "Time: 359.20637722700485 Epoch: 25 Iteration: 9126 Val Loss: 0.50941795 Val Acc: 0.8962339743589743\n",
      "Time: 354.69024018399796 Epoch: 26 Iteration: 9477 Val Loss: 0.51267207 Val Acc: 0.8952323717948718\n",
      "Safe best model\n",
      "Time: 357.8051496390035 Epoch: 27 Iteration: 9828 Val Loss: 0.50772315 Val Acc: 0.8958333333333334\n",
      "New learning rate: 0.0010000000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 354.538715607996 Epoch: 28 Iteration: 10179 Val Loss: 0.509103 Val Acc: 0.8968349358974359\n",
      "Safe best model\n",
      "Time: 357.9416380729963 Epoch: 29 Iteration: 10530 Val Loss: 0.5057621 Val Acc: 0.8984375\n",
      "Safe best model\n",
      "Time: 358.006988096 Epoch: 30 Iteration: 10881 Val Loss: 0.5050799 Val Acc: 0.8970352564102564\n",
      "Safe best model\n",
      "Time: 358.0606388249944 Epoch: 31 Iteration: 11232 Val Loss: 0.50419706 Val Acc: 0.8984375\n",
      "Safe best model\n",
      "Time: 358.03512892599974 Epoch: 32 Iteration: 11583 Val Loss: 0.5040655 Val Acc: 0.8998397435897436\n",
      "Safe best model\n",
      "Time: 357.93949780899857 Epoch: 33 Iteration: 11934 Val Loss: 0.50227076 Val Acc: 0.9002403846153846\n",
      "Safe best model\n",
      "Time: 358.07272951699997 Epoch: 34 Iteration: 12285 Val Loss: 0.50181866 Val Acc: 0.9006410256410257\n",
      "Time: 354.5665509620012 Epoch: 35 Iteration: 12636 Val Loss: 0.5020661 Val Acc: 0.8998397435897436\n",
      "Safe best model\n",
      "Time: 357.9736905000027 Epoch: 36 Iteration: 12987 Val Loss: 0.5006972 Val Acc: 0.8982371794871795\n",
      "Time: 354.67223265800567 Epoch: 37 Iteration: 13338 Val Loss: 0.5013408 Val Acc: 0.8982371794871795\n",
      "Time: 354.3863797839949 Epoch: 38 Iteration: 13689 Val Loss: 0.5026123 Val Acc: 0.8978365384615384\n",
      "Safe best model\n",
      "Time: 357.972723152001 Epoch: 39 Iteration: 14040 Val Loss: 0.5000604 Val Acc: 0.8996394230769231\n",
      "Time: 354.3829038690019 Epoch: 40 Iteration: 14391 Val Loss: 0.5008099 Val Acc: 0.8998397435897436\n",
      "Safe best model\n",
      "Time: 357.9589249920027 Epoch: 41 Iteration: 14742 Val Loss: 0.4994047 Val Acc: 0.8994391025641025\n",
      "Time: 354.65145645199664 Epoch: 42 Iteration: 15093 Val Loss: 0.4999787 Val Acc: 0.8984375\n",
      "Safe best model\n",
      "Time: 432.9523963219981 Epoch: 0 Iteration: 351 Val Loss: 1.9576386 Val Acc: 0.4935897435897436\n",
      "Safe best model\n",
      "Time: 358.1083601299979 Epoch: 1 Iteration: 702 Val Loss: 1.4147931 Val Acc: 0.6223958333333334\n",
      "Safe best model\n",
      "Time: 358.39230900500115 Epoch: 2 Iteration: 1053 Val Loss: 1.1590772 Val Acc: 0.6887019230769231\n",
      "Safe best model\n",
      "Time: 358.2029016719971 Epoch: 3 Iteration: 1404 Val Loss: 1.0688593 Val Acc: 0.7169471153846154\n",
      "Safe best model\n",
      "Time: 358.42371558099694 Epoch: 4 Iteration: 1755 Val Loss: 0.9725564 Val Acc: 0.7445913461538461\n",
      "Safe best model\n",
      "Time: 358.15092626000114 Epoch: 5 Iteration: 2106 Val Loss: 0.9283829 Val Acc: 0.7628205128205128\n",
      "Safe best model\n",
      "Time: 358.1123188030033 Epoch: 6 Iteration: 2457 Val Loss: 0.87875867 Val Acc: 0.7792467948717948\n",
      "Safe best model\n",
      "Time: 358.2512667810006 Epoch: 7 Iteration: 2808 Val Loss: 0.7651857 Val Acc: 0.8082932692307693\n",
      "Time: 354.8300522110003 Epoch: 8 Iteration: 3159 Val Loss: 0.7749074 Val Acc: 0.8086939102564102\n",
      "Time: 354.5911346829962 Epoch: 9 Iteration: 3510 Val Loss: 0.80517805 Val Acc: 0.8030849358974359\n",
      "Safe best model\n",
      "Time: 357.7693532119956 Epoch: 10 Iteration: 3861 Val Loss: 0.7281365 Val Acc: 0.8247195512820513\n",
      "Time: 354.7228357470012 Epoch: 11 Iteration: 4212 Val Loss: 0.7529947 Val Acc: 0.8193108974358975\n",
      "Safe best model\n",
      "Time: 357.8735797499976 Epoch: 12 Iteration: 4563 Val Loss: 0.67254424 Val Acc: 0.8465544871794872\n",
      "Safe best model\n",
      "Time: 358.16800252000394 Epoch: 13 Iteration: 4914 Val Loss: 0.6715953 Val Acc: 0.8435496794871795\n",
      "Time: 354.61478109500604 Epoch: 14 Iteration: 5265 Val Loss: 0.67284375 Val Acc: 0.844551282051282\n",
      "Time: 354.4701189589978 Epoch: 15 Iteration: 5616 Val Loss: 0.69840527 Val Acc: 0.8387419871794872\n",
      "Time: 354.5350537219929 Epoch: 16 Iteration: 5967 Val Loss: 0.693531 Val Acc: 0.8357371794871795\n",
      "Safe best model\n",
      "Time: 357.84799997200025 Epoch: 17 Iteration: 6318 Val Loss: 0.63848335 Val Acc: 0.8501602564102564\n",
      "Time: 354.68596927498584 Epoch: 18 Iteration: 6669 Val Loss: 0.6729912 Val Acc: 0.8465544871794872\n",
      "Time: 354.4306282969919 Epoch: 19 Iteration: 7020 Val Loss: 0.64299077 Val Acc: 0.8571714743589743\n",
      "Safe best model\n",
      "Time: 357.8361235399934 Epoch: 20 Iteration: 7371 Val Loss: 0.6128626 Val Acc: 0.8655849358974359\n",
      "New learning rate: 0.010000000000000002\n",
      "Safe best model\n",
      "Time: 358.2356069249945 Epoch: 21 Iteration: 7722 Val Loss: 0.5419402 Val Acc: 0.8886217948717948\n",
      "Safe best model\n",
      "Time: 358.0113982040057 Epoch: 22 Iteration: 8073 Val Loss: 0.527617 Val Acc: 0.8954326923076923\n",
      "Safe best model\n",
      "Time: 357.69754084300075 Epoch: 23 Iteration: 8424 Val Loss: 0.5273509 Val Acc: 0.8936298076923077\n",
      "Safe best model\n",
      "Time: 357.96692487600376 Epoch: 24 Iteration: 8775 Val Loss: 0.52183414 Val Acc: 0.8984375\n",
      "Safe best model\n",
      "Time: 357.75470789399697 Epoch: 25 Iteration: 9126 Val Loss: 0.509974 Val Acc: 0.9004407051282052\n",
      "Time: 354.4086371919984 Epoch: 26 Iteration: 9477 Val Loss: 0.51239544 Val Acc: 0.8988381410256411\n",
      "Time: 354.4714364199899 Epoch: 27 Iteration: 9828 Val Loss: 0.513365 Val Acc: 0.8994391025641025\n",
      "New learning rate: 0.0010000000000000002\n",
      "Safe best model\n",
      "Time: 358.96511213399936 Epoch: 28 Iteration: 10179 Val Loss: 0.5071809 Val Acc: 0.9030448717948718\n",
      "Safe best model\n",
      "Time: 358.7193535099941 Epoch: 29 Iteration: 10530 Val Loss: 0.50573575 Val Acc: 0.9032451923076923\n",
      "Safe best model\n",
      "Time: 358.78050101400004 Epoch: 30 Iteration: 10881 Val Loss: 0.5040952 Val Acc: 0.9034455128205128\n",
      "Time: 355.0514878540125 Epoch: 31 Iteration: 11232 Val Loss: 0.50437003 Val Acc: 0.9030448717948718\n",
      "Time: 354.91033134199097 Epoch: 32 Iteration: 11583 Val Loss: 0.5041489 Val Acc: 0.9026442307692307\n",
      "Safe best model\n",
      "Time: 358.20627570299257 Epoch: 33 Iteration: 11934 Val Loss: 0.5026725 Val Acc: 0.9042467948717948\n",
      "Time: 355.02213802799815 Epoch: 34 Iteration: 12285 Val Loss: 0.50382274 Val Acc: 0.9028445512820513\n",
      "Safe best model\n",
      "Time: 358.30428717999894 Epoch: 35 Iteration: 12636 Val Loss: 0.5017915 Val Acc: 0.9034455128205128\n",
      "Time: 355.1874402620015 Epoch: 36 Iteration: 12987 Val Loss: 0.5020254 Val Acc: 0.9034455128205128\n",
      "Time: 355.05899214699457 Epoch: 37 Iteration: 13338 Val Loss: 0.50238764 Val Acc: 0.9026442307692307\n",
      "Time: 355.14649789199757 Epoch: 38 Iteration: 13689 Val Loss: 0.5026644 Val Acc: 0.9026442307692307\n",
      "Safe best model\n",
      "Time: 358.67933529299626 Epoch: 39 Iteration: 14040 Val Loss: 0.50095886 Val Acc: 0.9026442307692307\n",
      "Safe best model\n",
      "Time: 358.60953201800294 Epoch: 40 Iteration: 14391 Val Loss: 0.50074005 Val Acc: 0.9032451923076923\n",
      "Time: 354.97058550300426 Epoch: 41 Iteration: 14742 Val Loss: 0.5009906 Val Acc: 0.9040464743589743\n",
      "Time: 355.3096559290134 Epoch: 42 Iteration: 15093 Val Loss: 0.50074047 Val Acc: 0.9034455128205128\n",
      "Safe best model\n",
      "Time: 431.81977250700584 Epoch: 0 Iteration: 351 Val Loss: 2.1291487 Val Acc: 0.4515224358974359\n",
      "Safe best model\n",
      "Time: 359.9220593959908 Epoch: 1 Iteration: 702 Val Loss: 1.2065747 Val Acc: 0.6594551282051282\n",
      "Safe best model\n",
      "Time: 360.9711084159935 Epoch: 2 Iteration: 1053 Val Loss: 1.2064692 Val Acc: 0.6792868589743589\n",
      "Safe best model\n",
      "Time: 361.21347166399937 Epoch: 3 Iteration: 1404 Val Loss: 1.0183879 Val Acc: 0.7279647435897436\n",
      "Safe best model\n",
      "Time: 360.52773742499994 Epoch: 4 Iteration: 1755 Val Loss: 0.90473664 Val Acc: 0.7696314102564102\n",
      "Safe best model\n",
      "Time: 361.01595672500844 Epoch: 5 Iteration: 2106 Val Loss: 0.86088276 Val Acc: 0.7862580128205128\n",
      "Safe best model\n",
      "Time: 361.1303191689949 Epoch: 6 Iteration: 2457 Val Loss: 0.8526486 Val Acc: 0.7872596153846154\n",
      "Safe best model\n",
      "Time: 360.15345679799793 Epoch: 7 Iteration: 2808 Val Loss: 0.78438747 Val Acc: 0.8076923076923077\n",
      "Safe best model\n",
      "Time: 360.0163282669964 Epoch: 8 Iteration: 3159 Val Loss: 0.77578807 Val Acc: 0.8062900641025641\n",
      "Safe best model\n",
      "Time: 359.7479431100073 Epoch: 9 Iteration: 3510 Val Loss: 0.7122168 Val Acc: 0.8307291666666666\n",
      "Time: 356.41189564099477 Epoch: 10 Iteration: 3861 Val Loss: 0.742577 Val Acc: 0.8243189102564102\n",
      "Safe best model\n",
      "Time: 360.2805882870016 Epoch: 11 Iteration: 4212 Val Loss: 0.6925856 Val Acc: 0.8369391025641025\n",
      "Time: 356.4996247779927 Epoch: 12 Iteration: 4563 Val Loss: 0.6961398 Val Acc: 0.8357371794871795\n",
      "Safe best model\n",
      "Time: 359.6725479750021 Epoch: 13 Iteration: 4914 Val Loss: 0.6542219 Val Acc: 0.8537660256410257\n",
      "Time: 356.64056467400223 Epoch: 14 Iteration: 5265 Val Loss: 0.6793604 Val Acc: 0.8423477564102564\n",
      "Safe best model\n",
      "Time: 360.4790287039941 Epoch: 15 Iteration: 5616 Val Loss: 0.64364076 Val Acc: 0.8577724358974359\n",
      "Time: 356.74359453500074 Epoch: 16 Iteration: 5967 Val Loss: 0.6593541 Val Acc: 0.852363782051282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 356.8664163570065 Epoch: 17 Iteration: 6318 Val Loss: 0.6466306 Val Acc: 0.8485576923076923\n",
      "Safe best model\n",
      "Time: 360.25066624599276 Epoch: 18 Iteration: 6669 Val Loss: 0.6276665 Val Acc: 0.8623798076923077\n",
      "Time: 357.2041127820121 Epoch: 19 Iteration: 7020 Val Loss: 0.6341772 Val Acc: 0.8609775641025641\n",
      "Time: 357.2190784500126 Epoch: 20 Iteration: 7371 Val Loss: 0.6366002 Val Acc: 0.8501602564102564\n",
      "New learning rate: 0.010000000000000002\n",
      "Safe best model\n",
      "Time: 360.6728262559918 Epoch: 21 Iteration: 7722 Val Loss: 0.538424 Val Acc: 0.8886217948717948\n",
      "Safe best model\n",
      "Time: 360.52387011999963 Epoch: 22 Iteration: 8073 Val Loss: 0.51694477 Val Acc: 0.8952323717948718\n",
      "Safe best model\n",
      "Time: 360.81180646699795 Epoch: 23 Iteration: 8424 Val Loss: 0.51477605 Val Acc: 0.8974358974358975\n",
      "Time: 357.3222439300007 Epoch: 24 Iteration: 8775 Val Loss: 0.5173221 Val Acc: 0.8964342948717948\n",
      "Safe best model\n",
      "Time: 360.4113720000023 Epoch: 25 Iteration: 9126 Val Loss: 0.5079282 Val Acc: 0.8984375\n",
      "Time: 357.1934353680117 Epoch: 26 Iteration: 9477 Val Loss: 0.5090783 Val Acc: 0.8978365384615384\n",
      "Time: 356.61237748200074 Epoch: 27 Iteration: 9828 Val Loss: 0.5128159 Val Acc: 0.8940304487179487\n",
      "New learning rate: 0.0010000000000000002\n",
      "Safe best model\n",
      "Time: 360.51323220199265 Epoch: 28 Iteration: 10179 Val Loss: 0.4998924 Val Acc: 0.9032451923076923\n",
      "Safe best model\n",
      "Time: 360.2048660989967 Epoch: 29 Iteration: 10530 Val Loss: 0.49903086 Val Acc: 0.9030448717948718\n",
      "Safe best model\n",
      "Time: 359.7805992670037 Epoch: 30 Iteration: 10881 Val Loss: 0.49816748 Val Acc: 0.9024439102564102\n",
      "Safe best model\n",
      "Time: 360.88144929100235 Epoch: 31 Iteration: 11232 Val Loss: 0.49589118 Val Acc: 0.9036458333333334\n",
      "Safe best model\n",
      "Time: 360.7481259100023 Epoch: 32 Iteration: 11583 Val Loss: 0.49538916 Val Acc: 0.9038461538461539\n",
      "Safe best model\n",
      "Time: 360.5721914750029 Epoch: 33 Iteration: 11934 Val Loss: 0.4951343 Val Acc: 0.9032451923076923\n",
      "Time: 356.7472058519925 Epoch: 34 Iteration: 12285 Val Loss: 0.49521196 Val Acc: 0.9042467948717948\n",
      "Safe best model\n",
      "Time: 360.1098256060068 Epoch: 35 Iteration: 12636 Val Loss: 0.49479753 Val Acc: 0.9030448717948718\n",
      "Time: 356.9323086240038 Epoch: 36 Iteration: 12987 Val Loss: 0.4949817 Val Acc: 0.9036458333333334\n",
      "Safe best model\n",
      "Time: 360.09455271300976 Epoch: 37 Iteration: 13338 Val Loss: 0.49466637 Val Acc: 0.9030448717948718\n",
      "Safe best model\n",
      "Time: 360.1936354920035 Epoch: 38 Iteration: 13689 Val Loss: 0.49351373 Val Acc: 0.9040464743589743\n",
      "Safe best model\n",
      "Time: 360.67145050599356 Epoch: 39 Iteration: 14040 Val Loss: 0.493134 Val Acc: 0.9040464743589743\n",
      "Time: 357.05009206500836 Epoch: 40 Iteration: 14391 Val Loss: 0.49363527 Val Acc: 0.9044471153846154\n",
      "Time: 357.08344892600144 Epoch: 41 Iteration: 14742 Val Loss: 0.49350485 Val Acc: 0.9046474358974359\n",
      "Safe best model\n",
      "Time: 360.52854076698713 Epoch: 42 Iteration: 15093 Val Loss: 0.4916162 Val Acc: 0.9032451923076923\n",
      "Safe best model\n",
      "Time: 435.03318435499386 Epoch: 0 Iteration: 351 Val Loss: 2.076493 Val Acc: 0.4671474358974359\n",
      "Safe best model\n",
      "Time: 360.6854925289954 Epoch: 1 Iteration: 702 Val Loss: 1.2473875 Val Acc: 0.6558493589743589\n",
      "Time: 357.3535124750051 Epoch: 2 Iteration: 1053 Val Loss: 1.271023 Val Acc: 0.6474358974358975\n",
      "Safe best model\n",
      "Time: 361.4740421139868 Epoch: 3 Iteration: 1404 Val Loss: 1.0417519 Val Acc: 0.7253605769230769\n",
      "Safe best model\n",
      "Time: 361.2032722180011 Epoch: 4 Iteration: 1755 Val Loss: 0.9606461 Val Acc: 0.7626201923076923\n",
      "Safe best model\n",
      "Time: 360.9765523820097 Epoch: 5 Iteration: 2106 Val Loss: 0.92688525 Val Acc: 0.7644230769230769\n",
      "Safe best model\n",
      "Time: 361.1480209709989 Epoch: 6 Iteration: 2457 Val Loss: 0.83831865 Val Acc: 0.7944711538461539\n"
     ]
    }
   ],
   "source": [
    "createPath('exp/densenet_variance')\n",
    "\n",
    "for no_model in range(1, 10):\n",
    "    createPath('exp/densenet_variance/' + str(no_model))\n",
    "    model = DenseNetModel(images, \n",
    "                          labels,\n",
    "                          batch_size = 128, \n",
    "                          max_iteration = 15000, \n",
    "                          lr = 0.1,\n",
    "                          lr_iteration_step = [7500, 10000],\n",
    "                          path = 'exp/densenet_variance/' + str(no_model)\n",
    "                         )\n",
    "    model.build_model()\n",
    "    model.train(images, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "papermill": {
   "duration": 64725.90102,
   "end_time": "2019-03-10T15:47:49.296116",
   "environment_variables": {},
   "exception": null,
   "input_path": "AlphaENAS.ipynb",
   "output_path": "AlphaENAS_DenseNetVariance_v2.ipynb",
   "parameters": {},
   "start_time": "2019-03-09T21:49:03.395096",
   "version": "0.19.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
